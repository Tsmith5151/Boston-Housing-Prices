{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named ggplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb15e15b4962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mggplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named ggplot"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import grid_search\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "################################\n",
    "### ADD EXTRA LIBRARIES HERE ###\n",
    "################################\n",
    "def selected_features():\n",
    "    # Selecting a few of the features (instead of using all) might improve performance.\n",
    "\n",
    "    return [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "\n",
    "def features_to_string():\n",
    "    return reduce(lambda x, y: x + ', ' + y, selected_features())\n",
    "\n",
    "def df_features(housing_features):\n",
    "        column_names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "\n",
    "        row_length, col_length = housing_features.shape\n",
    "        dictionary = {}\n",
    "\n",
    "        for col_i in range(0,col_length):\n",
    "          current_column_name = column_names[col_i]\n",
    "          values = []\n",
    "\n",
    "          for row_i in range(0,row_length):\n",
    "            current_feature_value = housing_features[row_i][col_i]\n",
    "            values.append(current_feature_value)\n",
    "\n",
    "          dictionary[current_column_name] = values\n",
    "\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "\n",
    "        return df[selected_features()]\n",
    "\n",
    "def load_data():\n",
    "        '''Load the Boston dataset.'''\n",
    "\n",
    "        boston = datasets.load_boston()\n",
    "        return boston\n",
    "\n",
    "\n",
    "def explore_city_data(city_data):\n",
    "        '''Calculate the Boston housing statistics.'''\n",
    "\n",
    "        # Get the labels and features from the housing data\n",
    "        housing_prices = city_data.target\n",
    "        housing_features = city_data.data\n",
    "\n",
    "        ###################################\n",
    "        ### Step 1. YOUR CODE GOES HERE ###\n",
    "        ###################################\n",
    "\n",
    "        # Please calculate the following values using the Numpy library\n",
    "        # Size of data?\n",
    "        # Number of features?\n",
    "        # Minimum value?\n",
    "        # Maximum Value?\n",
    "        # Calculate mean?\n",
    "        # Calculate median?\n",
    "        # Calculate standard deviation?\n",
    "\n",
    "        housing_series = pd.Series(housing_prices)\n",
    "\n",
    "        print \"housing_series.describe\"\n",
    "        print housing_series.describe()\n",
    "        print \"\\n\"\n",
    "\n",
    "        # housing_series.describe\n",
    "        # count    506.000000\n",
    "        # mean      22.532806\n",
    "        # std        9.197104\n",
    "        # min        5.000000\n",
    "        # 25%       17.025000\n",
    "        # 50%       21.200000\n",
    "        # 75%       25.000000\n",
    "        # max       50.000000\n",
    "\n",
    "        print \"housing_features.shape\"\n",
    "        print housing_features.shape\n",
    "        print \"\\n\"\n",
    "\n",
    "def performance_metric(label, prediction):\n",
    "\n",
    "        '''Calculate and return the appropriate performance metric.'''\n",
    "\n",
    "        ###################################\n",
    "        ### Step 2. YOUR CODE GOES HERE ###\n",
    "        ###################################\n",
    "\n",
    "        # http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
    "\n",
    "        return metrics.mean_squared_error(label, prediction)\n",
    "\n",
    "def split_data(city_data):\n",
    "        '''Randomly shuffle the sample set. Divide it into training and testing set.'''\n",
    "\n",
    "        # Get the features and labels from the Boston housing data\n",
    "        X, y = city_data.data, city_data.target\n",
    "\n",
    "        ###################################\n",
    "        ### Step 3. YOUR CODE GOES HERE ###\n",
    "        ###################################\n",
    "\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(df_features(X), y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # kf = KFold(n=506, n_folds=5, shuffle=True, random_state=True)\n",
    "\n",
    "        # for train_index, test_index in kf:\n",
    "           # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "           # X_train, X_test = X[train_index], X[test_index]\n",
    "           # y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def learning_curve(depth, X_train, y_train, X_test, y_test):\n",
    "        '''Calculate the performance of the model after a set of training data.'''\n",
    "\n",
    "        # We will vary the training set size so that we have 50 different sizes\n",
    "        sizes = np.linspace(1, len(X_train), 50)\n",
    "        train_err = np.zeros(len(sizes))\n",
    "        test_err = np.zeros(len(sizes))\n",
    "\n",
    "        print \"Decision Tree with Max Depth: \"\n",
    "        print depth\n",
    "\n",
    "        for i, s in enumerate(sizes):\n",
    "\n",
    "                # Create and fit the decision tree regressor model\n",
    "                regressor = DecisionTreeRegressor(max_depth=depth)\n",
    "                regressor.fit(X_train[:int(s)], y_train[:int(s)])\n",
    "\n",
    "                # Find the performance on the training and testing set\n",
    "                train_err[i] = performance_metric(y_train[:int(s)], regressor.predict(X_train[:int(s)]))\n",
    "                test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "\n",
    "        # Plot learning curve graph\n",
    "        learning_curve_graph(sizes, train_err, test_err, depth)\n",
    "\n",
    "\n",
    "def learning_curve_graph(sizes, train_err, test_err, depth):\n",
    "        '''Plot training and test error as a function of the training size.'''\n",
    "\n",
    "        pl.figure()\n",
    "        pl.title('Decision Trees: Performance vs Training Size - Depth ' + str(depth) + ', f: ' + features_to_string())\n",
    "        pl.plot(sizes, test_err, lw=2, label = 'test error')\n",
    "        pl.plot(sizes, train_err, lw=2, label = 'training error')\n",
    "        pl.legend()\n",
    "        pl.xlabel('Training Size')\n",
    "        pl.ylabel('Error')\n",
    "        pl.savefig('dt-perf-vs-ts-' + features_to_string())\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "def model_complexity(X_train, y_train, X_test, y_test):\n",
    "        '''Calculate the performance of the model as model complexity increases.'''\n",
    "\n",
    "        print \"Model Complexity: \"\n",
    "\n",
    "        # We will vary the depth of decision trees from 2 to 25\n",
    "        max_depth = np.arange(1, 25)\n",
    "        train_err = np.zeros(len(max_depth))\n",
    "        test_err = np.zeros(len(max_depth))\n",
    "\n",
    "        for i, d in enumerate(max_depth):\n",
    "                # Setup a Decision Tree Regressor so that it learns a tree with depth d\n",
    "                regressor = DecisionTreeRegressor(max_depth=d)\n",
    "\n",
    "                # Fit the learner to the training data\n",
    "                regressor.fit(X_train, y_train)\n",
    "\n",
    "                # Find the performance on the training set\n",
    "                train_err[i] = performance_metric(y_train, regressor.predict(X_train))\n",
    "\n",
    "                # Find the performance on the testing set\n",
    "                test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "        # Plot the model complexity graph\n",
    "        model_complexity_graph(max_depth, train_err, test_err)\n",
    "\n",
    "\n",
    "def model_complexity_graph(max_depth, train_err, test_err):\n",
    "        '''Plot training and test error as a function of the depth of the decision tree learn.'''\n",
    "\n",
    "        pl.figure()\n",
    "        pl.title('Decision Trees: Performance vs Max Depth, f: ' + features_to_string())\n",
    "        pl.plot(max_depth, test_err, lw=2, label = 'test error')\n",
    "        pl.plot(max_depth, train_err, lw=2, label = 'training error')\n",
    "        pl.legend()\n",
    "        pl.xlabel('Max Depth')\n",
    "        pl.ylabel('Error')\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "def fit_predict_model(city_data):\n",
    "        '''Find and tune the optimal model. Make a prediction on housing data.'''\n",
    "\n",
    "        # Get the features and labels from the Boston housing data\n",
    "        X, y = city_data.data, city_data.target\n",
    "\n",
    "        # Setup a Decision Tree Regressor\n",
    "        regressor = DecisionTreeRegressor()\n",
    "\n",
    "        parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}\n",
    "\n",
    "        ###################################\n",
    "        ### Step 4. YOUR CODE GOES HERE ###\n",
    "        ###################################\n",
    "\n",
    "        # 1. Find the best performance metric\n",
    "        # should be the same as your performance_metric procedure\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "\n",
    "        # 2. Use gridearch to fine tune the Decision Tree Regressor and find the best model\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV\n",
    "\n",
    "\n",
    "        reg = grid_search.GridSearchCV(regressor, parameters, scoring='mean_squared_error')\n",
    "        # Fit the learner to the training data\n",
    "        print \"Final Model: \"\n",
    "        print reg.fit(df_features(X), y)\n",
    "        # print reg.fit(X, y)\n",
    "\n",
    "    # Use the model to predict the output of a particular sample\n",
    "        df_x = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                'CRIM': [11.95],\n",
    "                'ZN': [0.00],\n",
    "                'INDUS': [18.100],\n",
    "                'CHAS': [0],\n",
    "                'NOX': [0.6590],\n",
    "                'RM': [5.6090],\n",
    "                'AGE': [90.00],\n",
    "                'DIS': [1.385],\n",
    "                'RAD': [24],\n",
    "                'TAX': [680.0],\n",
    "                'PTRATIO': [20.20],\n",
    "                'B': [332.09],\n",
    "                'LSTAT': [12.13] })\n",
    "\n",
    "        x = df_x[selected_features()]\n",
    "        print \"x \"\n",
    "        print x\n",
    "        print \"\\n\"\n",
    "\n",
    "        y = reg.predict(x)\n",
    "        print \"House: \" + str(x)\n",
    "        print \"Prediction: \" + str(y)\n",
    "\n",
    "        print \"reg.best_estimator_\"\n",
    "        print reg.best_estimator_\n",
    "        print \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "        '''Analyze the Boston housing data. Evaluate and validate the\n",
    "        performanance of a Decision Tree regressor on the Boston data.\n",
    "        Fine tune the model to make prediction on unseen data.'''\n",
    "\n",
    "        # Load data\n",
    "        city_data = load_data()\n",
    "\n",
    "        # Explore the data\n",
    "        explore_city_data(city_data)\n",
    "\n",
    "        # Training/Test dataset split\n",
    "        X_train, y_train, X_test, y_test = split_data(city_data)\n",
    "\n",
    "        # Learning Curve Graphs\n",
    "        max_depths = [1,2,3,4,5,6,7,8,9,10]\n",
    "        for max_depth in max_depths:\n",
    "            learning_curve(max_depth, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # Model Complexity Graph\n",
    "        model_complexity(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # Tune and predict Model\n",
    "        fit_predict_model(city_data)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
